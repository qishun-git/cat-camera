# Global cat face project configuration.
paths:
  # Root directory that holds all dataset folders.
  base_data_dir: data
  # Destination for unlabeled captures (may live under base_data_dir).
  unlabeled_dir: data/unlabeled
  # Destination for labeled training images.
  training_dir: data/training
  # Directory for rejected images when sorting.
  reject_dir: data/rejected
  # Directory that stores trained models and label maps.
  models_dir: models

vision:
  # Primary camera index shared by capture and recognition.
  camera_index: 1
  # Size (pixels) used for preprocessing/training/recognition.
  face_size: 224
  # Prefer Picamera2 on Raspberry Pi (falls back to OpenCV if unavailable).
  prefer_picamera2: false
  # Resolution for Picamera2 capture (width, height).
  picamera_resolution: [1280, 720]
  # Target FPS for Picamera2 capture.
  picamera_fps: 30

detection:
  # Path to a YOLO ONNX model (relative paths resolve from repo root).
  model: pretrained_models/yolov5n.onnx
  # Input image size for YOLO preprocessing (single value or [width, height]).
  input_size: 640
  # YOLO class IDs to keep (COCO cat is 15); empty list keeps all.
  class_ids: [15]
  # Minimum confidence score for YOLO detections.
  conf_threshold: 0.5
  # IOU threshold used during YOLO non-max suppression.
  iou_threshold: 0.45
  # Optional ONNX Runtime providers (e.g., ["CPUExecutionProvider"]).
  providers: []

sorter:
  # Title for the sorting window.
  window_name: Sort Unlabeled
  # Sorter window width in pixels.
  window_width: 640
  # Sorter window height in pixels.
  window_height: 480
  # Initial X coordinate for the sorter window.
  window_x: 100
  # Initial Y coordinate for the sorter window.
  window_y: 100
  # Whether rejects are deleted from disk.
  delete_rejects: false
  # Image extensions the sorter will display.
  image_extensions: [".png", ".jpg", ".jpeg"]
  # Optional per-label cap applied after sorting (0 or null to disable).
  per_label_limit: 1000

training:
  # Label map filename saved under paths.models_dir.
  labels_filename: labels.json
  # Filename for the embedding-based recognizer (produced by train_embeddings.py).
  embedding_model_filename: embeddings.npz
  # Input size used by the embedding backbone (must match train/recognize).
  embedding_input_size: 224

recognition:
  # Embedding cosine similarity threshold when using the embedding method.
  embedding_threshold: 0.8
  # Filename for the embedding centroids (produced by train_embeddings.py).
  embedding_model_filename: embeddings.npz
  # Label map filename (defaults to training.labels_filename).
  labels_filename: labels.json

recorder:
  # Directory where detection-triggered video clips are saved (relative paths resolve from repo base).
  output_dir: data/clips
  # Minimum clip length in seconds (clips shorter than this are discarded).
  min_duration: 15.0
  # Maximum clip length in seconds (recording stops automatically once reached).
  max_duration: 300.0
  # Cooldown time in seconds before another clip can be recorded.
  cooldown: 5.0
  # Grace period (seconds) after the last detection before ending a clip.
  absence_grace: 5.0
  # Preferred output FPS (0 or null falls back to camera FPS).
  fps: 15
  # FourCC codec used by OpenCV VideoWriter (default mp4v -> MP4-friendly capture).
  codec: mp4v
  # How often to run detection (seconds); 0 = every frame. Default 0.5s (~2 Hz).
  detection_interval: 0.5

streaming:
  # MJPEG streamer settings shared between the recorder and web UI.
  # Interface and port for the local streamer (running inside record_cat_video).
  host: 0.0.0.0
  port: 8765
  # Resolution for the stream preview (width, height). Leave null to reuse capture size.
  resolution: [640, 360]
  # JPEG quality for the preview (10-95).
  quality: 80
  # Minimum delay between streamed frames (seconds). 0 for as fast as possible.
  frame_interval: 0.03
  # Public URL that the web app should embed (typically http://<pi-ip>:port/stream.mjpg).
  public_url: http://localhost:8765/stream.mjpg
  # Overlay detection boxes onto the streamed frames.
  show_annotations: true
  # File where the recorder writes viewer count for the web UI.
  status_path: tmp/stream_status.json

clip_processing:
  # Directory containing recorded clips to process (defaults to recorder.output_dir).
  clips_dir: data/compressed_clips
  # Optional cap on the number of faces saved per clip (null/0 for unlimited).
  save_limit: 100
  # Number of frames to promote into training when a cat is auto-recognized.
  training_refresh_count: 10
  # Additional margin above recognition threshold required for automatic recognition.
  recognition_margin: 0
  # CRF used when re-encoding clips to H.265 (lower = higher quality/bigger files).
  compression_crf: 28
  # Seconds between watcher polling cycles.
  watch_interval: 5.0
